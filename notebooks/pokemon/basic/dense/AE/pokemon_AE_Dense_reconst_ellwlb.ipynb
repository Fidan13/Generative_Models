{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_KERAS=1\n",
      "\\\n"
     ]
    }
   ],
   "source": [
    "%env TF_KERAS = 1\n",
    "import os\n",
    "sep_local = os.path.sep\n",
    "\n",
    "import sys\n",
    "sys.path.append('..'+sep_local+'..')\n",
    "print(sep_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid\\Documents\\projects\\Generative_Models\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..'+sep_local+'..'+sep_local+'..'+sep_local+'..'+sep_local+'..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat data/.pokemon/pokemon.tar.part* > data/.pokemon/pokemon.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvf data/.pokemon/pokemon_combined.tar --directory data/.pokemon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from training.generators.from_lmdb.lmdb_image_generator import get_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='pokemon'\n",
    "inputs_shape= image_shape=(100, 100, 3)\n",
    "batch_size = 10\n",
    "latents_dim = 32\n",
    "intermediate_dim = 50\n",
    "lmdb_dir = 'data/.pokemon/Pokemon_LMDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformation.lmdb_transformer import LmdbTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEBUG    | Looking for images in '_training'\n",
      "  DEBUG    | Looking for images in '_training'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING  | No files found\n",
      "  WARNING  | No files found\n",
      "C:\\Users\\Khalid\\Documents\\projects\\Generative_Models\\transformation\\file_image_generator.py:51: UserWarning: No files found\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEBUG    | Looking for images in '_validation'\n",
      "  DEBUG    | Looking for images in '_validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING  | No files found\n",
      "  WARNING  | No files found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterator training Number of images 591\n",
      "Initializing Iterator validation Number of images 218\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator = get_generators(\n",
    "        lmdb_dir=lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=None,\n",
    "        episode_shift=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "_instance_scale=1.0\n",
    "for data in val_generator:\n",
    "    print(data['images'].numpy().max())\n",
    "    break\n",
    "    #print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['images'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'label'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's Layers definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "if isinstance(inputs_shape, Iterable):\n",
    "    flat_outputs_shape = np.prod(inputs_shape)\n",
    "flat_outputs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_lays = [tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=latents_dim)]\n",
    "\n",
    "dec_lays = [tf.keras.layers.Dense(units=latents_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units= latents_dim*intermediate_dim//3 , activation='relu'),\n",
    "            tf.keras.layers.Dense(units= latents_dim*intermediate_dim//3 , activation='relu'),\n",
    "\n",
    "            tf.keras.layers.Dense(units=flat_outputs_shape),\n",
    "            tf.keras.layers.Reshape(inputs_shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = dataset_name+'AE_Dense_reconst_ell'\n",
    "experiments_dir='experiments'+sep_local+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from training.autoencoding_basic.autoencoders.autoencoder import autoencoder as AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_params = \\\n",
    "[\n",
    "    {\n",
    "        'name': 'inference', \n",
    "        'inputs_shape':inputs_shape,\n",
    "        'outputs_shape':latents_dim,\n",
    "        'layers': enc_lays\n",
    "    }\n",
    "\n",
    "    ,\n",
    "    \n",
    "        {\n",
    "        'name': 'generative', \n",
    "        'inputs_shape':latents_dim,\n",
    "        'outputs_shape':inputs_shape,\n",
    "        'layers':dec_lays\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_restore = os.path.join(experiments_dir, 'var_save_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments\\\\pokemonAE_Dense_reconst_ell\\\\var_save_dir'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_if_not_exist(_restore)\n",
    "_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to restore trained model, set filepath=_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inference\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inference_inputs (InputLayer [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 100, 50)      200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 100, 50)      2550      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 500000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                16000032  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activity_regularization (Act (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "inference_outputs (Activatio (None, 32)                0         \n",
      "=================================================================\n",
      "Total params: 16,002,910\n",
      "Trainable params: 16,002,846\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING  | None\n",
      "  WARNING  | None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generative\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generative_inputs (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 533)               17589     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 533)               284622    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 30000)             16020000  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 3)       12        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "activity_regularization_1 (A (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "generative_outputs (Activati (None, 100, 100, 3)       0         \n",
      "=================================================================\n",
      "Total params: 16,323,279\n",
      "Trainable params: 16,323,273\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING  | None\n",
      "  WARNING  | None\n"
     ]
    }
   ],
   "source": [
    "ae = AE( \n",
    "    name=model_name,\n",
    "    latents_dim=latents_dim,\n",
    "    batch_size=batch_size,\n",
    "    variables_params=variables_params, \n",
    "    filepath=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pokemonAE_Dense_reconst_ell\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inference_inputs (InputLayer [(None, 100, 100, 3)]     0         \n",
      "_________________________________________________________________\n",
      "inference (Functional)       (None, 32)                16002910  \n",
      "_________________________________________________________________\n",
      "generative (Functional)      (None, 100, 100, 3)       16323279  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_x_logits (Tensor [(None, 100, 100, 3)]     0         \n",
      "=================================================================\n",
      "Total params: 32,326,189\n",
      "Trainable params: 32,326,119\n",
      "Non-trainable params: 70\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ae.compile(metrics=None)\n",
    "#ae.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100, 100, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.callbacks.sample_generation import SampleGeneration\n",
    "from training.callbacks.save_model import ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    min_delta=1e-12, \n",
    "    patience=6, \n",
    "    verbose=1, \n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ModelSaver(filepath=_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments\\\\pokemonAE_Dense_reconst_ell\\\\csv_dir\\\\pokemonAE_Dense_reconst_ell.csv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = os.path.join(experiments_dir, 'csv_dir')\n",
    "create_if_not_exist(csv_dir)\n",
    "csv_dir = os.path.join(csv_dir, ae.name+'.csv')\n",
    "csv_log = tf.keras.callbacks.CSVLogger(csv_dir, append=True)\n",
    "csv_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_dir = os.path.join(experiments_dir, 'image_gen_dir')\n",
    "create_if_not_exist(image_gen_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = SampleGeneration(latents_shape=latents_dim, filepath=image_gen_dir, gen_freq=5, save_img=True, gray_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000\n",
      "    2/10000 [..............................] - ETA: 3:49 - loss: 16713.750 - ETA: 9:53 - loss: 18206.0039WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0249s vs `on_train_batch_end` time: 0.0708s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  274/10000 [..............................] - ETA: 11:44 - loss: 18735.61 - ETA: 12:37 - loss: 18467.97 - ETA: 13:04 - loss: 17277.52 - ETA: 13:23 - loss: 16718.92 - ETA: 13:31 - loss: 16372.99 - ETA: 13:37 - loss: 15943.15 - ETA: 13:42 - loss: 15638.04 - ETA: 13:47 - loss: 15558.17 - ETA: 13:50 - loss: 15454.09 - ETA: 13:56 - loss: 15319.77 - ETA: 14:03 - loss: 15175.02 - ETA: 14:08 - loss: 15076.03 - ETA: 14:11 - loss: 14817.08 - ETA: 14:13 - loss: 14814.01 - ETA: 14:17 - loss: 14782.22 - ETA: 14:19 - loss: 14676.00 - ETA: 14:20 - loss: 14660.32 - ETA: 14:21 - loss: 14727.63 - ETA: 14:21 - loss: 14635.46 - ETA: 14:21 - loss: 14606.87 - ETA: 14:22 - loss: 14636.53 - ETA: 14:23 - loss: 14643.82 - ETA: 14:23 - loss: 14643.28 - ETA: 14:23 - loss: 14547.84 - ETA: 14:24 - loss: 14470.84 - ETA: 14:24 - loss: 14373.83 - ETA: 14:25 - loss: 14316.57 - ETA: 14:25 - loss: 14340.10 - ETA: 14:26 - loss: 14349.68 - ETA: 14:26 - loss: 14326.80 - ETA: 14:26 - loss: 14319.12 - ETA: 14:26 - loss: 14272.74 - ETA: 14:27 - loss: 14210.79 - ETA: 14:27 - loss: 14160.39 - ETA: 14:28 - loss: 14137.98 - ETA: 14:28 - loss: 14075.50 - ETA: 14:28 - loss: 14067.95 - ETA: 14:28 - loss: 14047.02 - ETA: 14:28 - loss: 14019.10 - ETA: 14:28 - loss: 13981.47 - ETA: 14:28 - loss: 13920.98 - ETA: 14:28 - loss: 13887.87 - ETA: 14:28 - loss: 13898.07 - ETA: 14:28 - loss: 13862.15 - ETA: 14:28 - loss: 13853.41 - ETA: 14:28 - loss: 13849.68 - ETA: 14:28 - loss: 13783.69 - ETA: 14:29 - loss: 13759.13 - ETA: 14:29 - loss: 13764.40 - ETA: 14:29 - loss: 13765.73 - ETA: 14:29 - loss: 13737.61 - ETA: 14:29 - loss: 13741.64 - ETA: 14:29 - loss: 13692.31 - ETA: 14:29 - loss: 13671.21 - ETA: 14:29 - loss: 13611.19 - ETA: 14:29 - loss: 13542.00 - ETA: 14:29 - loss: 13535.45 - ETA: 14:29 - loss: 13506.20 - ETA: 14:29 - loss: 13510.10 - ETA: 14:29 - loss: 13481.61 - ETA: 14:29 - loss: 13464.04 - ETA: 14:30 - loss: 13422.97 - ETA: 14:30 - loss: 13444.53 - ETA: 14:30 - loss: 13455.13 - ETA: 14:30 - loss: 13443.05 - ETA: 14:31 - loss: 13420.52 - ETA: 14:31 - loss: 13423.77 - ETA: 14:31 - loss: 13409.36 - ETA: 14:31 - loss: 13394.28 - ETA: 14:31 - loss: 13377.50 - ETA: 14:31 - loss: 13390.09 - ETA: 14:31 - loss: 13369.51 - ETA: 14:31 - loss: 13361.21 - ETA: 14:31 - loss: 13338.14 - ETA: 14:30 - loss: 13327.39 - ETA: 14:30 - loss: 13300.35 - ETA: 14:30 - loss: 13290.70 - ETA: 14:30 - loss: 13278.98 - ETA: 14:30 - loss: 13263.75 - ETA: 14:30 - loss: 13234.96 - ETA: 14:30 - loss: 13200.15 - ETA: 14:30 - loss: 13203.51 - ETA: 14:30 - loss: 13186.61 - ETA: 14:30 - loss: 13167.11 - ETA: 14:30 - loss: 13160.63 - ETA: 14:29 - loss: 13149.41 - ETA: 14:29 - loss: 13127.89 - ETA: 14:29 - loss: 13125.14 - ETA: 14:29 - loss: 13104.44 - ETA: 14:29 - loss: 13085.43 - ETA: 14:29 - loss: 13102.97 - ETA: 14:29 - loss: 13095.09 - ETA: 14:28 - loss: 13088.06 - ETA: 14:28 - loss: 13096.52 - ETA: 14:28 - loss: 13091.84 - ETA: 14:28 - loss: 13073.02 - ETA: 14:27 - loss: 13072.77 - ETA: 14:27 - loss: 13059.30 - ETA: 14:27 - loss: 13063.60 - ETA: 14:27 - loss: 13055.61 - ETA: 14:27 - loss: 13063.24 - ETA: 14:27 - loss: 13050.37 - ETA: 14:27 - loss: 13074.35 - ETA: 14:26 - loss: 13051.85 - ETA: 14:26 - loss: 13034.01 - ETA: 14:26 - loss: 13025.18 - ETA: 14:26 - loss: 13003.82 - ETA: 14:26 - loss: 13012.87 - ETA: 14:26 - loss: 13008.26 - ETA: 14:26 - loss: 12979.66 - ETA: 14:26 - loss: 12976.68 - ETA: 14:26 - loss: 12971.54 - ETA: 14:26 - loss: 12972.14 - ETA: 14:25 - loss: 12949.98 - ETA: 14:25 - loss: 12949.72 - ETA: 14:25 - loss: 12929.44 - ETA: 14:25 - loss: 12931.99 - ETA: 14:25 - loss: 12934.49 - ETA: 14:25 - loss: 12922.09 - ETA: 14:24 - loss: 12919.29 - ETA: 14:24 - loss: 12930.13 - ETA: 14:24 - loss: 12934.10 - ETA: 14:24 - loss: 12922.69 - ETA: 14:24 - loss: 12912.26 - ETA: 14:24 - loss: 12903.41 - ETA: 14:24 - loss: 12883.41 - ETA: 14:24 - loss: 12877.64 - ETA: 14:24 - loss: 12856.37 - ETA: 14:24 - loss: 12861.59 - ETA: 14:24 - loss: 12848.56 - ETA: 14:24 - loss: 12833.89 - ETA: 14:23 - loss: 12822.72 - ETA: 14:23 - loss: 12814.84 - ETA: 14:23 - loss: 12816.75 - ETA: 14:23 - loss: 12804.56 - ETA: 14:23 - loss: 12795.93 - ETA: 14:23 - loss: 12807.96 - ETA: 14:23 - loss: 12811.63 - ETA: 14:23 - loss: 12814.68 - ETA: 14:22 - loss: 12801.20 - ETA: 14:22 - loss: 12804.93 - ETA: 14:22 - loss: 12806.12 - ETA: 14:22 - loss: 12803.12 - ETA: 14:22 - loss: 12790.87 - ETA: 14:22 - loss: 12786.06 - ETA: 14:21 - loss: 12777.20 - ETA: 14:21 - loss: 12768.80 - ETA: 14:21 - loss: 12757.54 - ETA: 14:21 - loss: 12757.60 - ETA: 14:21 - loss: 12757.52 - ETA: 14:21 - loss: 12753.70 - ETA: 14:21 - loss: 12749.68 - ETA: 14:21 - loss: 12738.34 - ETA: 14:20 - loss: 12724.72 - ETA: 14:20 - loss: 12721.56 - ETA: 14:20 - loss: 12713.87 - ETA: 14:20 - loss: 12719.93 - ETA: 14:20 - loss: 12710.96 - ETA: 14:20 - loss: 12705.27 - ETA: 14:20 - loss: 12708.74 - ETA: 14:20 - loss: 12705.95 - ETA: 14:20 - loss: 12703.28 - ETA: 14:20 - loss: 12701.04 - ETA: 14:19 - loss: 12702.89 - ETA: 14:19 - loss: 12692.34 - ETA: 14:19 - loss: 12694.52 - ETA: 14:19 - loss: 12691.04 - ETA: 14:19 - loss: 12683.71 - ETA: 14:19 - loss: 12673.12 - ETA: 14:19 - loss: 12668.45 - ETA: 14:18 - loss: 12662.36 - ETA: 14:18 - loss: 12660.69 - ETA: 14:18 - loss: 12651.18 - ETA: 14:18 - loss: 12649.81 - ETA: 14:18 - loss: 12643.08 - ETA: 14:18 - loss: 12643.09 - ETA: 14:18 - loss: 12637.25 - ETA: 14:18 - loss: 12632.04 - ETA: 14:17 - loss: 12622.46 - ETA: 14:17 - loss: 12616.82 - ETA: 14:17 - loss: 12612.45 - ETA: 14:17 - loss: 12607.47 - ETA: 14:17 - loss: 12598.09 - ETA: 14:17 - loss: 12600.59 - ETA: 14:17 - loss: 12589.80 - ETA: 14:17 - loss: 12582.48 - ETA: 14:17 - loss: 12577.17 - ETA: 14:16 - loss: 12579.13 - ETA: 14:16 - loss: 12575.18 - ETA: 14:16 - loss: 12574.56 - ETA: 14:16 - loss: 12565.29 - ETA: 14:16 - loss: 12562.84 - ETA: 14:16 - loss: 12555.16 - ETA: 14:16 - loss: 12549.11 - ETA: 14:16 - loss: 12553.82 - ETA: 14:16 - loss: 12549.33 - ETA: 14:15 - loss: 12542.77 - ETA: 14:15 - loss: 12532.88 - ETA: 14:15 - loss: 12523.38 - ETA: 14:15 - loss: 12512.09 - ETA: 14:15 - loss: 12515.34 - ETA: 14:15 - loss: 12512.70 - ETA: 14:15 - loss: 12498.31 - ETA: 14:15 - loss: 12493.73 - ETA: 14:15 - loss: 12485.82 - ETA: 14:14 - loss: 12480.45 - ETA: 14:14 - loss: 12481.85 - ETA: 14:14 - loss: 12486.49 - ETA: 14:14 - loss: 12477.70 - ETA: 14:14 - loss: 12487.62 - ETA: 14:14 - loss: 12493.63 - ETA: 14:14 - loss: 12494.69 - ETA: 14:14 - loss: 12493.02 - ETA: 14:14 - loss: 12496.60 - ETA: 14:14 - loss: 12494.88 - ETA: 14:13 - loss: 12491.48 - ETA: 14:13 - loss: 12491.67 - ETA: 14:13 - loss: 12481.57 - ETA: 14:13 - loss: 12478.10 - ETA: 14:13 - loss: 12478.28 - ETA: 14:13 - loss: 12468.76 - ETA: 14:13 - loss: 12465.81 - ETA: 14:13 - loss: 12472.24 - ETA: 14:13 - loss: 12477.57 - ETA: 14:13 - loss: 12477.24 - ETA: 14:12 - loss: 12470.06 - ETA: 14:12 - loss: 12465.44 - ETA: 14:12 - loss: 12466.31 - ETA: 14:12 - loss: 12459.24 - ETA: 14:12 - loss: 12447.09 - ETA: 14:12 - loss: 12447.24 - ETA: 14:12 - loss: 12443.28 - ETA: 14:12 - loss: 12440.56 - ETA: 14:12 - loss: 12439.17 - ETA: 14:11 - loss: 12438.66 - ETA: 14:11 - loss: 12446.47 - ETA: 14:11 - loss: 12440.24 - ETA: 14:11 - loss: 12429.93 - ETA: 14:11 - loss: 12432.67 - ETA: 14:11 - loss: 12430.32 - ETA: 14:11 - loss: 12425.78 - ETA: 14:11 - loss: 12423.12 - ETA: 14:10 - loss: 12418.65 - ETA: 14:10 - loss: 12418.61 - ETA: 14:10 - loss: 12418.79 - ETA: 14:10 - loss: 12414.35 - ETA: 14:10 - loss: 12416.49 - ETA: 14:10 - loss: 12421.91 - ETA: 14:10 - loss: 12422.00 - ETA: 14:10 - loss: 12416.63 - ETA: 14:09 - loss: 12418.32 - ETA: 14:09 - loss: 12415.92 - ETA: 14:09 - loss: 12412.62 - ETA: 14:09 - loss: 12413.76 - ETA: 14:09 - loss: 12404.40 - ETA: 14:09 - loss: 12403.05 - ETA: 14:08 - loss: 12393.77 - ETA: 14:08 - loss: 12396.32 - ETA: 14:08 - loss: 12390.95 - ETA: 14:08 - loss: 12388.02 - ETA: 14:08 - loss: 12382.81 - ETA: 14:07 - loss: 12383.67 - ETA: 14:07 - loss: 12381.76 - ETA: 14:07 - loss: 12379.49 - ETA: 14:07 - loss: 12371.42 - ETA: 14:07 - loss: 12369.88 - ETA: 14:07 - loss: 12368.77 - ETA: 14:07 - loss: 12366.63 - ETA: 14:06 - loss: 12363.13 - ETA: 14:06 - loss: 12357.18 - ETA: 14:06 - loss: 12352.64 - ETA: 14:06 - loss: 12346.1660"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  540/10000 [>.............................] - ETA: 14:06 - loss: 12346.95 - ETA: 14:06 - loss: 12345.78 - ETA: 14:06 - loss: 12336.42 - ETA: 14:05 - loss: 12332.77 - ETA: 14:05 - loss: 12329.16 - ETA: 14:05 - loss: 12328.77 - ETA: 14:05 - loss: 12326.05 - ETA: 14:05 - loss: 12321.28 - ETA: 14:05 - loss: 12321.80 - ETA: 14:04 - loss: 12331.08 - ETA: 14:04 - loss: 12330.85 - ETA: 14:04 - loss: 12333.87 - ETA: 14:04 - loss: 12323.15 - ETA: 14:04 - loss: 12314.63 - ETA: 14:04 - loss: 12312.68 - ETA: 14:04 - loss: 12306.89 - ETA: 14:03 - loss: 12304.49 - ETA: 14:03 - loss: 12304.75 - ETA: 14:03 - loss: 12297.67 - ETA: 14:03 - loss: 12292.18 - ETA: 14:03 - loss: 12286.67 - ETA: 14:03 - loss: 12275.32 - ETA: 14:03 - loss: 12274.23 - ETA: 14:03 - loss: 12269.98 - ETA: 14:03 - loss: 12268.91 - ETA: 14:03 - loss: 12264.98 - ETA: 14:02 - loss: 12263.54 - ETA: 14:02 - loss: 12260.49 - ETA: 14:02 - loss: 12259.64 - ETA: 14:02 - loss: 12255.83 - ETA: 14:02 - loss: 12257.13 - ETA: 14:02 - loss: 12253.09 - ETA: 14:02 - loss: 12246.03 - ETA: 14:01 - loss: 12240.55 - ETA: 14:01 - loss: 12234.88 - ETA: 14:01 - loss: 12231.58 - ETA: 14:01 - loss: 12229.13 - ETA: 14:01 - loss: 12224.18 - ETA: 14:01 - loss: 12218.54 - ETA: 14:01 - loss: 12212.84 - ETA: 14:01 - loss: 12206.13 - ETA: 14:01 - loss: 12213.27 - ETA: 14:00 - loss: 12208.11 - ETA: 14:00 - loss: 12202.43 - ETA: 14:00 - loss: 12199.34 - ETA: 14:00 - loss: 12198.61 - ETA: 14:00 - loss: 12197.42 - ETA: 14:00 - loss: 12189.27 - ETA: 14:00 - loss: 12186.60 - ETA: 14:00 - loss: 12183.95 - ETA: 13:59 - loss: 12181.89 - ETA: 13:59 - loss: 12177.73 - ETA: 13:59 - loss: 12178.49 - ETA: 13:59 - loss: 12182.67 - ETA: 13:59 - loss: 12184.86 - ETA: 13:59 - loss: 12183.04 - ETA: 13:59 - loss: 12178.40 - ETA: 13:59 - loss: 12173.73 - ETA: 13:58 - loss: 12171.60 - ETA: 13:58 - loss: 12167.61 - ETA: 13:58 - loss: 12165.58 - ETA: 13:58 - loss: 12168.85 - ETA: 13:58 - loss: 12165.43 - ETA: 13:58 - loss: 12157.65 - ETA: 13:58 - loss: 12159.47 - ETA: 13:57 - loss: 12155.67 - ETA: 13:57 - loss: 12155.01 - ETA: 13:57 - loss: 12158.40 - ETA: 13:57 - loss: 12154.25 - ETA: 13:57 - loss: 12152.68 - ETA: 13:57 - loss: 12145.79 - ETA: 13:57 - loss: 12143.67 - ETA: 13:57 - loss: 12138.70 - ETA: 13:57 - loss: 12135.04 - ETA: 13:57 - loss: 12136.65 - ETA: 13:56 - loss: 12138.39 - ETA: 13:56 - loss: 12133.26 - ETA: 13:56 - loss: 12135.27 - ETA: 13:56 - loss: 12134.66 - ETA: 13:56 - loss: 12130.63 - ETA: 13:56 - loss: 12125.84 - ETA: 13:56 - loss: 12118.23 - ETA: 13:56 - loss: 12117.00 - ETA: 13:55 - loss: 12119.87 - ETA: 13:55 - loss: 12119.01 - ETA: 13:55 - loss: 12118.98 - ETA: 13:55 - loss: 12118.82 - ETA: 13:55 - loss: 12114.67 - ETA: 13:55 - loss: 12110.61 - ETA: 13:55 - loss: 12103.87 - ETA: 13:55 - loss: 12102.10 - ETA: 13:55 - loss: 12100.30 - ETA: 13:54 - loss: 12094.15 - ETA: 13:54 - loss: 12096.07 - ETA: 13:54 - loss: 12094.06 - ETA: 13:54 - loss: 12090.90 - ETA: 13:54 - loss: 12087.88 - ETA: 13:54 - loss: 12084.72 - ETA: 13:54 - loss: 12081.23 - ETA: 13:54 - loss: 12076.70 - ETA: 13:54 - loss: 12076.50 - ETA: 13:53 - loss: 12073.17 - ETA: 13:53 - loss: 12076.61 - ETA: 13:53 - loss: 12074.04 - ETA: 13:53 - loss: 12074.37 - ETA: 13:53 - loss: 12074.44 - ETA: 13:53 - loss: 12072.06 - ETA: 13:53 - loss: 12067.97 - ETA: 13:53 - loss: 12066.41 - ETA: 13:53 - loss: 12061.49 - ETA: 13:53 - loss: 12059.80 - ETA: 13:53 - loss: 12054.45 - ETA: 13:53 - loss: 12050.32 - ETA: 13:53 - loss: 12045.01 - ETA: 13:53 - loss: 12041.04 - ETA: 13:52 - loss: 12034.73 - ETA: 13:52 - loss: 12031.80 - ETA: 13:52 - loss: 12033.46 - ETA: 13:52 - loss: 12029.35 - ETA: 13:52 - loss: 12031.73 - ETA: 13:52 - loss: 12028.70 - ETA: 13:52 - loss: 12020.96 - ETA: 13:52 - loss: 12018.65 - ETA: 13:52 - loss: 12014.23 - ETA: 13:52 - loss: 12010.02 - ETA: 13:52 - loss: 12008.50 - ETA: 13:52 - loss: 12006.43 - ETA: 13:52 - loss: 12005.76 - ETA: 13:52 - loss: 12002.18 - ETA: 13:52 - loss: 12009.86 - ETA: 13:52 - loss: 12008.23 - ETA: 13:51 - loss: 12004.81 - ETA: 13:51 - loss: 12002.45 - ETA: 13:51 - loss: 12002.13 - ETA: 13:51 - loss: 12001.33 - ETA: 13:51 - loss: 11995.96 - ETA: 13:51 - loss: 11996.54 - ETA: 13:51 - loss: 11992.50 - ETA: 13:51 - loss: 11994.21 - ETA: 13:51 - loss: 11988.01 - ETA: 13:51 - loss: 11982.09 - ETA: 13:51 - loss: 11981.97 - ETA: 13:51 - loss: 11977.60 - ETA: 13:50 - loss: 11968.71 - ETA: 13:50 - loss: 11965.54 - ETA: 13:50 - loss: 11959.58 - ETA: 13:50 - loss: 11957.63 - ETA: 13:50 - loss: 11952.34 - ETA: 13:50 - loss: 11946.47 - ETA: 13:50 - loss: 11945.93 - ETA: 13:50 - loss: 11940.78 - ETA: 13:50 - loss: 11936.25 - ETA: 13:50 - loss: 11931.76 - ETA: 13:50 - loss: 11933.25 - ETA: 13:50 - loss: 11928.87 - ETA: 13:50 - loss: 11927.45 - ETA: 13:50 - loss: 11926.45 - ETA: 13:50 - loss: 11928.57 - ETA: 13:50 - loss: 11927.64 - ETA: 13:50 - loss: 11926.37 - ETA: 13:50 - loss: 11924.37 - ETA: 13:50 - loss: 11922.72 - ETA: 13:50 - loss: 11921.32 - ETA: 13:50 - loss: 11921.64 - ETA: 13:50 - loss: 11916.81 - ETA: 13:49 - loss: 11911.81 - ETA: 13:49 - loss: 11912.81 - ETA: 13:49 - loss: 11908.61 - ETA: 13:49 - loss: 11904.19 - ETA: 13:49 - loss: 11902.34 - ETA: 13:49 - loss: 11903.53 - ETA: 13:49 - loss: 11903.06 - ETA: 13:49 - loss: 11900.22 - ETA: 13:49 - loss: 11897.27 - ETA: 13:49 - loss: 11896.04 - ETA: 13:49 - loss: 11896.98 - ETA: 13:49 - loss: 11895.48 - ETA: 13:49 - loss: 11898.76 - ETA: 13:49 - loss: 11892.83 - ETA: 13:48 - loss: 11893.31 - ETA: 13:48 - loss: 11893.30 - ETA: 13:48 - loss: 11894.72 - ETA: 13:48 - loss: 11888.28 - ETA: 13:48 - loss: 11883.29 - ETA: 13:48 - loss: 11878.31 - ETA: 13:48 - loss: 11878.54 - ETA: 13:48 - loss: 11874.98 - ETA: 13:48 - loss: 11870.51 - ETA: 13:48 - loss: 11867.12 - ETA: 13:48 - loss: 11863.24 - ETA: 13:48 - loss: 11859.39 - ETA: 13:48 - loss: 11861.28 - ETA: 13:48 - loss: 11861.86 - ETA: 13:48 - loss: 11859.34 - ETA: 13:48 - loss: 11858.39 - ETA: 13:48 - loss: 11855.62 - ETA: 13:48 - loss: 11852.47 - ETA: 13:48 - loss: 11849.54 - ETA: 13:48 - loss: 11845.96 - ETA: 13:48 - loss: 11841.02 - ETA: 13:48 - loss: 11838.40 - ETA: 13:47 - loss: 11833.00 - ETA: 13:47 - loss: 11832.02 - ETA: 13:47 - loss: 11840.74 - ETA: 13:47 - loss: 11836.89 - ETA: 13:47 - loss: 11833.88 - ETA: 13:47 - loss: 11831.22 - ETA: 13:47 - loss: 11829.10 - ETA: 13:47 - loss: 11825.44 - ETA: 13:47 - loss: 11824.20 - ETA: 13:47 - loss: 11819.98 - ETA: 13:47 - loss: 11817.67 - ETA: 13:47 - loss: 11812.91 - ETA: 13:47 - loss: 11811.72 - ETA: 13:47 - loss: 11808.30 - ETA: 13:46 - loss: 11805.94 - ETA: 13:46 - loss: 11805.68 - ETA: 13:46 - loss: 11803.44 - ETA: 13:46 - loss: 11801.73 - ETA: 13:46 - loss: 11798.23 - ETA: 13:46 - loss: 11799.60 - ETA: 13:46 - loss: 11794.28 - ETA: 13:46 - loss: 11791.11 - ETA: 13:46 - loss: 11791.27 - ETA: 13:46 - loss: 11794.60 - ETA: 13:46 - loss: 11791.55 - ETA: 13:46 - loss: 11790.28 - ETA: 13:46 - loss: 11788.20 - ETA: 13:46 - loss: 11784.65 - ETA: 13:46 - loss: 11785.49 - ETA: 13:46 - loss: 11780.71 - ETA: 13:45 - loss: 11781.21 - ETA: 13:45 - loss: 11782.10 - ETA: 13:45 - loss: 11779.45 - ETA: 13:45 - loss: 11780.11 - ETA: 13:45 - loss: 11778.12 - ETA: 13:45 - loss: 11777.91 - ETA: 13:45 - loss: 11774.95 - ETA: 13:45 - loss: 11771.73 - ETA: 13:45 - loss: 11767.46 - ETA: 13:44 - loss: 11764.20 - ETA: 13:44 - loss: 11760.58 - ETA: 13:44 - loss: 11757.35 - ETA: 13:44 - loss: 11752.96 - ETA: 13:44 - loss: 11747.94 - ETA: 13:44 - loss: 11747.72 - ETA: 13:44 - loss: 11747.31 - ETA: 13:44 - loss: 11743.71 - ETA: 13:44 - loss: 11740.59 - ETA: 13:43 - loss: 11738.76 - ETA: 13:43 - loss: 11734.92 - ETA: 13:43 - loss: 11732.81 - ETA: 13:43 - loss: 11729.59 - ETA: 13:43 - loss: 11725.01 - ETA: 13:43 - loss: 11720.77 - ETA: 13:43 - loss: 11721.34 - ETA: 13:43 - loss: 11717.37 - ETA: 13:43 - loss: 11713.55 - ETA: 13:42 - loss: 11711.22 - ETA: 13:42 - loss: 11709.77 - ETA: 13:42 - loss: 11709.25 - ETA: 13:42 - loss: 11707.68 - ETA: 13:42 - loss: 11706.95 - ETA: 13:42 - loss: 11706.65 - ETA: 13:42 - loss: 11703.02 - ETA: 13:42 - loss: 11701.2695"
     ]
    }
   ],
   "source": [
    "ae.fit(\n",
    "    x=train_generator,\n",
    "    input_kw=None,\n",
    "    steps_per_epoch=int(1e4),\n",
    "    epochs=int(1e6), \n",
    "    verbose=1,\n",
    "    #callbacks=[ es,  ms, csv_log, sg],\n",
    "    workers=-1,\n",
    "    use_multiprocessing=True,\n",
    "    #validation_data=val_generator,\n",
    "    #validation_steps=int(1e4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.inception_metrics import inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_mean, is_sigma = inception_score(ae, tolerance_threshold=1e-6, max_iteration=100)\n",
    "print(f'inception_score mean: {is_mean}, sigma: {is_sigma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frechet_inception_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.inception_metrics import frechet_inception_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_score = frechet_inception_distance(ae, training_generator, tolerance_threshold=1e-6, max_iteration=100, batch_size=32)\n",
    "print(f'frechet inception distance: {fis_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptual_path_length_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.perceptual_path_length import perceptual_path_length_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_mean_score = perceptual_path_length_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=100, batch_size=32)\n",
    "print(f'perceptual path length score: {ppl_mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.precision_recall import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_precision_score = precision_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=100)\n",
    "print(f'precision score: {_precision_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.precision_recall import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recall_score = recall_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=100)\n",
    "print(f'recall score: {_recall_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import reconstruct_from_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'reconstruct_training_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "reconstruct_from_a_batch(ae, training_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'reconstruct_testing_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "reconstruct_from_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import generate_images_like_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'generate_training_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_like_a_batch(ae, training_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'generate_testing_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_like_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import generate_images_randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'random_synthetic_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_randomly(ae, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import interpolate_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'interpolate_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "interpolate_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
